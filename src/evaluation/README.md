# Fact-It Evaluation Framework

Automated evaluation system for testing and optimizing AI models and prompts for fact-checking.

## ðŸ“š Table of Contents

- [Quick Start](#quick-start)
- [Overview](#overview)
- [Installation](#installation)
- [Usage](#usage)
- [Datasets](#datasets)
- [Metrics](#metrics)
- [Architecture](#architecture)
- [Advanced Usage](#advanced-usage)

## Quick Start

**Goal**: Evaluate and improve your fact-checking prompts in 5 minutes.

```bash
# 1. Set API key
export OPENAI_API_KEY="your-key"

# 2. Run example evaluation (creates test data automatically)
npm run eval:example

# 3. Test production prompts
npm run eval:production

# 4. Compare prompt variants
npm run eval:compare
```

**Cost**: ~$0.0001 for example dataset, ~$0.50-$2 for real datasets (500-1000 samples)

## Overview

The evaluation framework enables systematic testing of:
- **Stage 1**: Claim detection (does text contain verifiable claims?)
- **Stage 2**: Claim verification (is the claim true/false/unknown?)
- **Models**: GPT-4o, Claude, Gemini, etc. (via Vercel AI SDK)
- **Prompts**: Multiple variants for A/B testing

### What Makes It Powerful

âœ… **Same Stack** - Uses Vercel AI SDK (same as extension)
âœ… **Multi-Provider** - OpenAI, Anthropic, Google out of the box
âœ… **Real Data** - Import AVeriTeC (4.5K) or FEVER (185K) datasets
âœ… **Comprehensive Metrics** - Accuracy, precision, recall, calibration, cost, latency
âœ… **Type Safe** - Full TypeScript + Zod validation

## Installation

Already installed! The framework is integrated into the main project.

```bash
# Verify installation
npm run eval:example
```

## Usage

### 1. Run Example Evaluation

Creates test datasets and runs basic evaluation:

```bash
npm run eval:example
```

**Output**: Stage 1 and Stage 2 metrics with sample predictions.

### 2. Evaluate Production Prompts

Test the actual prompts used in your extension:

```bash
npm run eval:production
```

**What it does**:
- Extracts production prompts from `src/background/ai/`
- Runs against test dataset
- Shows metrics + recommendations

### 3. Compare Prompt Variants

A/B test different prompt formulations:

```bash
npm run eval:compare
```

**What it does**:
- Tests 4+ prompt variants (baseline, detailed, conservative, etc.)
- Shows side-by-side comparison
- Recommends best performer

### 4. Generate Synthetic Dataset

Create test data using GPT-4o (for rapid prototyping):

```bash
npm run eval:generate 100  # Generate 100 samples
```

### 5. Import External Datasets

Use real human-annotated data for high-quality evaluation:

#### AVeriTeC (Recommended - Real-world claims)

```bash
# Download
git clone https://github.com/MichSchli/AVeriTeC.git

# Import 500 balanced samples
npm run eval:import averitec AVeriTeC/data/train.json \
  --max-samples 500 \
  --balance-labels \
  --output datasets/stage2_averitec_500.json
```

#### FEVER (Large-scale - Wikipedia claims)

```bash
# Download
wget https://s3-eu-west-1.amazonaws.com/fever.public/train.jsonl

# Import 1000 balanced samples
npm run eval:import fever train.jsonl \
  --max-samples 1000 \
  --balance-labels \
  --output datasets/stage2_fever_1k.json
```

**See [datasets/README.md](../../datasets/README.md) for full dataset documentation.**

## Datasets

### Current Datasets

- **stage1_example.json** (3 samples) - Stage 1 test data
- **stage2_example.json** (2 samples) - Stage 2 test data
- **stage1_synthetic.json** (10+ samples) - Generated by GPT-4o

### External Datasets (Recommended)

| Dataset | Size | Type | Best For |
|---------|------|------|----------|
| **AVeriTeC** | 4,568 | Real-world claims + web sources | Production evaluation (most realistic) |
| **FEVER** | 185,445 | Wikipedia claims + evidence | Large-scale benchmarking |

### Dataset Schema

**Stage 1 (Claim Detection)**:
```typescript
{
  id: string;
  text: string;
  platform: 'twitter' | 'linkedin' | 'facebook' | 'article';
  hasClaim: boolean;
  claims: string[];
  annotator: string;
  confidence: number;
}
```

**Stage 2 (Verification)**:
```typescript
{
  id: string;
  claim: string;
  verdict: 'true' | 'false' | 'unknown';
  confidence: number;
  sources: Source[];
  explanation: string;
  difficulty: 'easy' | 'medium' | 'hard';
  topic: 'politics' | 'health' | 'science' | 'business' | 'other';
}
```

## Metrics

### Stage 1 (Claim Detection)

**Primary Metrics**:
- **Precision**: Of detected claims, how many are real? (target: >90%)
- **Recall**: Of real claims, how many were detected? (target: >80%)
- **F1 Score**: Harmonic mean of precision/recall (target: >87%)
- **FPR**: False positive rate (target: <15%)

**Performance Metrics**:
- P90 Latency (target: <1s)
- Cost per sample
- Total cost

**Example Output**:
```
Stage 1 Evaluation Report: GPT-4o-mini
======================================
Accuracy:  100.0%
Precision: 100.0%
Recall:    100.0%
F1 Score:  100.0%
FPR:       0.0%

Mean Latency: 0.52s
Total Cost:   $0.0001
```

### Stage 2 (Verification)

**Primary Metrics**:
- **Accuracy**: Overall correctness (target: >80%)
- **Critical Errors**: TRUEâ†”FALSE swaps (target: <5%)
- **Calibration (ECE)**: Confidence vs accuracy (target: <0.10)

**Per-Class Metrics**:
- Precision, Recall, F1 for each verdict (true/false/unknown)

**Source Quality**:
- Average sources per prediction
- Source reliability score
- Source overlap with ground truth

## Architecture

### Project Structure

```
src/evaluation/
â”œâ”€â”€ types/
â”‚   â””â”€â”€ dataset-schema.ts      # Zod schemas + TypeScript types
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ dataset-manager.ts     # Load, split, filter datasets
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model-runner.ts        # Vercel AI SDK integration
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ evaluators.ts          # Metrics calculation
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ prompt-registry.ts     # Prompt template management
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ averitec-adapter.ts    # AVeriTeC dataset transformer
â”‚   â””â”€â”€ fever-adapter.ts       # FEVER dataset transformer
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ compare-prompts.ts     # A/B testing script
â”‚   â”œâ”€â”€ evaluate-production-prompts.ts
â”‚   â”œâ”€â”€ generate-synthetic-dataset.ts
â”‚   â””â”€â”€ import-external-dataset.ts
â””â”€â”€ examples/
    â””â”€â”€ example-evaluation.ts  # Interactive demo
```

### Key Components

#### 1. Model Runner

Unified interface for all AI providers using Vercel AI SDK:

```typescript
const runner = new ModelRunner();
const predictions = await runner.runBatch(
  'gpt-4o-mini',              // Model
  systemPrompt,               // Prompt
  samples                     // Test data
);
```

**Supported Models**:
- **OpenAI**: gpt-4o-mini, gpt-4o, o1-preview, o1-mini
- **Anthropic**: claude-3-5-sonnet, claude-3-opus, claude-3-haiku
- **Google**: gemini-1.5-flash, gemini-1.5-pro

#### 2. Evaluators

Calculate comprehensive metrics:

```typescript
const evaluator = new Stage1Evaluator();
const metrics = evaluator.evaluate(predictions, testData);

printEvaluationReport(1, metrics, 'GPT-4o-mini');
```

#### 3. Prompt Registry

Centralized prompt management with versioning:

```typescript
const registry = new PromptRegistry();
const prompt = registry.getPrompt('stage1_baseline', 1);
```

**Available Prompts**:
- Stage 1: `baseline`, `detailed_v1`, `conservative_v1`
- Stage 2: `baseline`, `detailed_v1`, `conservative_v1`

## Advanced Usage

### Custom Evaluation Script

```typescript
import {
  DatasetManager,
  ModelRunner,
  Stage1Evaluator,
  PromptRegistry,
  printEvaluationReport,
} from '@/evaluation';

// Setup
const manager = new DatasetManager('./datasets');
const testData = manager.loadStage1('stage1_dataset.json');

const runner = new ModelRunner();
const registry = new PromptRegistry();
const evaluator = new Stage1Evaluator();

// Run evaluation
const prompt = registry.getPrompt('stage1_baseline', 1);
const predictions = await runner.runBatch(
  'gpt-4o-mini',
  prompt!.systemPrompt,
  testData
);

// Calculate metrics
const metrics = evaluator.evaluate(predictions, testData);
printEvaluationReport(1, metrics, 'GPT-4o-mini');
```

### Compare Multiple Models

```typescript
const models = ['gpt-4o-mini', 'gpt-4o', 'claude-3-5-sonnet-20241022'];

for (const model of models) {
  const predictions = await runner.runBatch(model, prompt.systemPrompt, testData);
  const metrics = evaluator.evaluate(predictions, testData);

  console.log(
    `${model}: F1=${metrics.f1Score.toFixed(3)}, ` +
    `Cost=$${metrics.totalCost.toFixed(4)}, ` +
    `Latency=${metrics.p90Latency.toFixed(2)}s`
  );
}
```

### A/B Test Prompts

```typescript
const promptIds = ['stage1_baseline', 'stage1_detailed_v1', 'stage1_conservative_v1'];

for (const promptId of promptIds) {
  const prompt = registry.getPrompt(promptId, 1);
  const predictions = await runner.runBatch('gpt-4o-mini', prompt!.systemPrompt, testData);
  const metrics = evaluator.evaluate(predictions, testData);

  console.log(
    `${promptId}: ` +
    `Precision=${metrics.precision.toFixed(3)}, ` +
    `FPR=${metrics.falsePositiveRate.toFixed(3)}`
  );
}
```

### Dataset Filtering

```typescript
const manager = new DatasetManager('./datasets');

// Load and filter
const allData = manager.loadStage1('stage1_dataset.json');
const twitterClaims = manager.getSubset(1, {
  platform: 'twitter',
  hasClaim: true,
});

// Train/val/test split
const splits = manager.trainValTestSplit(1, {
  train: 0.7,
  val: 0.15,
  test: 0.15,
  stratifyBy: 'hasClaim',  // Maintain label distribution
  randomSeed: 42,
});
```

### Cost Estimation

```typescript
const runner = new ModelRunner();

// Estimate before running
const estimate = runner.estimateCost('gpt-4o-mini', 500, 300, 100);
console.log(`Estimated cost: $${estimate.totalCost.toFixed(2)}`);
```

## Workflow Recommendations

### Quick Iteration (< 5 minutes, ~$0.50)

```bash
# 1. Generate or use example data
npm run eval:example

# 2. Test current prompts
npm run eval:production

# 3. Compare variants
npm run eval:compare
```

### Production Evaluation (30 minutes, ~$5)

```bash
# 1. Import real dataset
npm run eval:import averitec path/to/data.json \
  --max-samples 500 --balance-labels

# 2. Comprehensive testing
npm run eval:production
npm run eval:compare

# 3. Test multiple models (edit scripts)
# - gpt-4o-mini (fast/cheap)
# - gpt-4o (high quality)
# - claude-3-5-sonnet (alternative)
```

### Continuous Improvement Cycle

```
1. Annotate dataset (100-500 samples)
   â†“
2. Run baseline evaluation
   â†“
3. Analyze error patterns
   â†“
4. Create improved prompt variant
   â†“
5. A/B test variants
   â†“
6. Deploy winner to extension
   â†“
7. Monitor production performance
   â†“
8. Repeat with more data
```

## Cost Reference

| Dataset Size | Model | Cost per Run |
|--------------|-------|--------------|
| 100 samples | GPT-4o-mini | ~$0.10 |
| 500 samples | GPT-4o-mini | ~$0.50 |
| 1000 samples | GPT-4o-mini | ~$1.00 |
| 100 samples | GPT-4o | ~$1.50 |
| 500 samples | GPT-4o | ~$7.50 |
| 1000 samples | GPT-4o | ~$15.00 |

**Tip**: Start with GPT-4o-mini for iteration, validate final choice with GPT-4o.

## Troubleshooting

### "No test dataset found"
```bash
npm run eval:example  # Creates example datasets
```

### "OPENAI_API_KEY not set"
```bash
export OPENAI_API_KEY="your-key"
```

### "Module not found"
```bash
npm install  # Reinstall dependencies
```

### Low accuracy / high error rate
1. Check dataset quality (spot check 10-20 samples)
2. Review error analysis in evaluation output
3. Try different prompt variants
4. Consider using a stronger model (GPT-4o vs mini)

### Unexpected costs
1. Use `--max-samples` to limit dataset size
2. Start with GPT-4o-mini before trying GPT-4o
3. Check cost estimates before running

## Additional Resources

- **[Dataset Documentation](../../datasets/README.md)** - Comprehensive dataset guide
- **[External Datasets Integration](../../docs/EXTERNAL_DATASETS_INTEGRATION.md)** - AVeriTeC/FEVER import details
- **[Design Document](../../docs/2025-10-18-evaluation-framework-design.md)** - Full technical specification
- **[Example Script](./examples/example-evaluation.ts)** - Interactive demonstration

## Contributing

To add new features:

1. **New Model Provider**: Add to `model-runner.ts` using AI SDK
2. **New Dataset Adapter**: Create in `adapters/` following existing patterns
3. **New Metrics**: Extend evaluators in `evaluation/evaluators.ts`
4. **New Prompts**: Register in `prompts/prompt-registry.ts`

## Questions?

- Run the example: `npm run eval:example`
- Check dataset docs: [datasets/README.md](../../datasets/README.md)
- Review examples: `src/evaluation/examples/example-evaluation.ts`
