# Fact-It Evaluation Datasets

This directory contains datasets for evaluating the fact-checking system.

## Current Datasets

### Hand-Crafted Examples

- **`stage1_example.json`** (3 samples) - Small test dataset for Stage 1 (claim detection)
- **`stage2_example.json`** (2 samples) - Small test dataset for Stage 2 (verification)

### Synthetic Dataset

- **`stage1_synthetic.json`** (10+ samples) - Generated by GPT-4o for rapid prototyping

## Importing External Datasets

We provide adapters for well-established fact-checking benchmarks:

### 1. AVeriTeC Dataset (Recommended)

**What it is**: Real-world claims verified against web sources with question-answer pairs and justifications.

**Best for**: Stage 2 verification evaluation (most realistic for our use case)

**Stats**:
- 4,568 real-world claims
- Labels: Supported, Refuted, Not Enough Evidence, Conflicting Evidence
- Evidence: Web URLs with excerpts + Q&A pairs
- Source: https://github.com/MichSchli/AVeriTeC

**How to import**:

```bash
# 1. Download AVeriTeC dataset
git clone https://github.com/MichSchli/AVeriTeC.git
cd AVeriTeC

# 2. Import training data (500 samples, balanced)
npm run eval:import averitec ./path/to/averitec/data/train.json \
  --max-samples 500 \
  --balance-labels \
  --output datasets/stage2_averitec_500.json

# 3. Import full dataset
npm run eval:import averitec ./path/to/averitec/data/train.json \
  --output datasets/stage2_averitec_full.json

# 4. Import without unknown verdicts
npm run eval:import averitec ./path/to/averitec/data/train.json \
  --exclude-unknown \
  --balance-labels \
  --output datasets/stage2_averitec_definitive.json
```

**Field mappings**:
```
AVeriTeC          →  Stage2Sample
─────────────────────────────────────
claim             →  claim
label             →  verdict (true/false/unknown)
justification     →  explanation
questions/answers →  sources (with URLs and excerpts)
claim_date        →  metadata.claim_date
speaker           →  metadata.speaker
```

### 2. FEVER Dataset

**What it is**: Claims verified against Wikipedia with evidence sentences.

**Best for**: Large-scale Stage 2 evaluation (established benchmark)

**Stats**:
- 185,445 claims
- Labels: SUPPORTS, REFUTES, NOT ENOUGH INFO
- Evidence: Wikipedia articles + sentence IDs
- Source: https://fever.ai/dataset/fever.html

**How to import**:

```bash
# 1. Download FEVER dataset
wget https://s3-eu-west-1.amazonaws.com/fever.public/train.jsonl

# 2. Import 1000 samples (balanced)
npm run eval:import fever ./train.jsonl \
  --max-samples 1000 \
  --balance-labels \
  --output datasets/stage2_fever_1k.json

# 3. Import only supported/refuted (exclude "not enough info")
npm run eval:import fever ./train.jsonl \
  --max-samples 2000 \
  --exclude-unknown \
  --balance-labels \
  --output datasets/stage2_fever_definitive.json
```

**Field mappings**:
```
FEVER            →  Stage2Sample
──────────────────────────────────
id               →  id (prefixed with "fever_")
claim            →  claim
label            →  verdict (SUPPORTS→true, REFUTES→false, NOT ENOUGH INFO→unknown)
evidence         →  sources (Wikipedia URLs)
```

**Note**: FEVER doesn't include the actual evidence text, only Wikipedia page titles and sentence IDs.

## Import Script Options

```bash
npm run eval:import <dataset> <input-file> [options]

Options:
  --output <path>         Output file path
  --max-samples <n>       Limit number of samples
  --exclude-unknown       Remove "unknown" verdict samples
  --balance-labels        Balance true/false/unknown distribution
  --min-sources <n>       Require minimum sources per sample
```

## Recommended Workflow

### Quick Start (30 minutes)

```bash
# 1. Import AVeriTeC (500 samples)
npm run eval:import averitec ./data/averitec_train.json \
  --max-samples 500 \
  --balance-labels \
  --output datasets/stage2_test.json

# 2. Evaluate production prompts
npm run eval:production

# 3. Compare prompt variants
npm run eval:compare
```

### Full Evaluation (2-3 hours)

```bash
# 1. Import AVeriTeC (full dataset)
npm run eval:import averitec ./data/averitec_train.json \
  --output datasets/stage2_averitec_full.json

# 2. Import FEVER (2000 samples for comparison)
npm run eval:import fever ./train.jsonl \
  --max-samples 2000 \
  --balance-labels \
  --output datasets/stage2_fever_2k.json

# 3. Run comprehensive evaluation
npm run eval:production
npm run eval:compare

# 4. Test multiple models
# Edit scripts to use different models (gpt-4o, claude-3-5-sonnet, etc.)
```

## Dataset Statistics

After importing, you'll see statistics like:

```
Dataset Statistics:
============================================================
Total samples: 500

Verdict distribution:
  true      : 200 (40.0%)
  false     : 200 (40.0%)
  unknown   : 100 (20.0%)

Topic distribution:
  politics  : 150 (30.0%)
  health    : 100 (20.0%)
  science   : 80 (16.0%)
  business  : 70 (14.0%)
  other     : 100 (20.0%)

Difficulty distribution:
  easy      : 150 (30.0%)
  medium    : 250 (50.0%)
  hard      : 100 (20.0%)

Average sources per sample: 2.3
============================================================
```

## Dataset Quality Guidelines

### For Stage 1 (Claim Detection)

Good samples have:
- ✅ Clear distinction between claims and opinions
- ✅ Diverse platforms (Twitter, Facebook, LinkedIn)
- ✅ Mix of topics
- ✅ Balanced labels (50% with claims, 50% without)

### For Stage 2 (Verification)

Good samples have:
- ✅ At least 1 reliable source (2+ preferred)
- ✅ Clear verdict with justification
- ✅ Balanced labels (equal true/false, fewer unknown)
- ✅ Diverse topics and difficulty levels

## Cost Estimates

Running evaluation on different dataset sizes:

| Dataset Size | Model       | Cost (Stage 2) |
|--------------|-------------|----------------|
| 100 samples  | GPT-4o-mini | ~$0.10         |
| 500 samples  | GPT-4o-mini | ~$0.50         |
| 1000 samples | GPT-4o-mini | ~$1.00         |
| 100 samples  | GPT-4o      | ~$1.50         |
| 500 samples  | GPT-4o      | ~$7.50         |
| 1000 samples | GPT-4o      | ~$15.00        |

**Tip**: Start with 100-500 samples and GPT-4o-mini for rapid iteration.

## Troubleshooting

### "Input file not found"

Make sure you've downloaded the dataset first:

```bash
# AVeriTeC
git clone https://github.com/MichSchli/AVeriTeC.git

# FEVER
wget https://s3-eu-west-1.amazonaws.com/fever.public/train.jsonl
```

### "No valid samples after transformation"

Check the import logs for validation errors. Common issues:
- Missing sources (use `--min-sources 0` to allow)
- Invalid URLs in sources
- Unexpected label formats

### "Out of memory"

For large datasets (100k+ samples), import in batches:

```bash
# Import first 5000
npm run eval:import fever ./train.jsonl --max-samples 5000 --output datasets/stage2_fever_batch1.json

# Import next 5000 (manually offset in the file)
# Or use the --balance-labels flag which samples intelligently
```

## Contributing New Datasets

To add support for a new dataset:

1. Create adapter: `src/evaluation/adapters/mydataset-adapter.ts`
2. Implement transform function matching `Stage1Sample` or `Stage2Sample` schema
3. Update import script: `src/evaluation/scripts/import-external-dataset.ts`
4. Document in this README

See existing adapters ([averitec-adapter.ts](../src/evaluation/adapters/averitec-adapter.ts), [fever-adapter.ts](../src/evaluation/adapters/fever-adapter.ts)) for examples.

## Dataset Licenses

- **AVeriTeC**: CC BY-NC 4.0 (Non-commercial use)
- **FEVER**: Open (check original repository for details)

Always cite the original papers when publishing results:

**AVeriTeC**:
```
@article{schlichtkrull2023averitec,
  title={AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web},
  author={Schlichtkrull, Michael and others},
  journal={arXiv preprint arXiv:2305.13117},
  year={2023}
}
```

**FEVER**:
```
@inproceedings{thorne2018fever,
  title={FEVER: a large-scale dataset for Fact Extraction and VERification},
  author={Thorne, James and others},
  booktitle={NAACL-HLT},
  year={2018}
}
```

## Questions?

See:
- [Evaluation Framework README](../src/evaluation/README.md) - Comprehensive guide with quick start, usage, and examples
- [External Datasets Integration](../docs/EXTERNAL_DATASETS_INTEGRATION.md) - Technical details on AVeriTeC/FEVER adapters
- [Design Document](../docs/2025-10-18-evaluation-framework-design.md) - Original design specification
